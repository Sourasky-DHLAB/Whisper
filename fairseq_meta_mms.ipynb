{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sourasky-DHLAB/Whisper/blob/main/fairseq_meta_mms.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K97c9UMoA7GZ"
      },
      "outputs": [],
      "source": [
        "# clone fairseq\n",
        "import os\n",
        "\n",
        "!git clone https://github.com/pytorch/fairseq\n",
        "\n",
        "# Get the current working directory\n",
        "current_dir = os.getcwd()\n",
        "\n",
        "# Create the directory paths\n",
        "audio_samples_dir = os.path.join(current_dir, \"audio_samples\")\n",
        "temp_dir = os.path.join(current_dir, \"temp_dir\")\n",
        "\n",
        "# Create the directories if they don't exist\n",
        "os.makedirs(audio_samples_dir, exist_ok=True)\n",
        "os.makedirs(temp_dir, exist_ok=True)\n",
        "\n",
        "\n",
        "# Change current working directory\n",
        "os.chdir('fairseq')\n",
        "\n",
        "!pwd\n",
        "\n",
        "# to install the latest stable release (0.10.x)\n",
        "# pip install fairseq"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Install requirements and build\n",
        "\n",
        "!pip install --editable ./"
      ],
      "metadata": {
        "id": "Y8CduZvkB3uc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install tensorboardX\n",
        "!pip install tensorboardX"
      ],
      "metadata": {
        "id": "0udc5SQSEw2g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pwd"
      ],
      "metadata": {
        "id": "uaADAq4TOG-b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download Model (uncomment)\n",
        "\n",
        "# # MMS-1B:FL102 model - 102 Languages - FLEURS Dataset\n",
        "#!wget -P ./models_new 'https://dl.fbaipublicfiles.com/mms/asr/mms1b_fl102.pt'\n",
        "\n",
        "# # MMS-1B:L1107 - 1107 Languages - MMS-lab Dataset\n",
        "#!wget -P ./models_new 'https://dl.fbaipublicfiles.com/mms/asr/mms1b_l1107.pt'\n",
        "\n",
        "# MMS-1B-all - 1162 Languages - MMS-lab + FLEURS + CV + VP + MLS\n",
        "!wget -P ./models_new 'https://dl.fbaipublicfiles.com/mms/asr/mms1b_all.pt'\n"
      ],
      "metadata": {
        "id": "ixIToCXuE-XV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Run Inference for a short audio file only\n",
        "\n",
        "import os\n",
        "import time\n",
        "\n",
        "os.environ[\"TMPDIR\"] = '/content/temp_dir'\n",
        "os.environ[\"PYTHONPATH\"] = \".\"\n",
        "os.environ[\"PREFIX\"] = \"INFER\"\n",
        "os.environ[\"HYDRA_FULL_ERROR\"] = \"1\"\n",
        "os.environ[\"USER\"] = \"micro\"\n",
        "start_time = time.time()  # Start the timer\n",
        "\n",
        "!python examples/mms/asr/infer/mms_infer.py --model \"/content/fairseq/models_new/mms1b_fl102.pt\" --lang \"heb\" --audio \"/content/audio_samples/t0102.wav\"\n",
        "\n",
        "end_time = time.time()  # Stop the timer\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "print(\"Elapsed Time:\", elapsed_time, \"seconds\")"
      ],
      "metadata": {
        "id": "Aw0XFCOnE-U2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pydub"
      ],
      "metadata": {
        "id": "rgZvxpaptkfC",
        "outputId": "02655ffd-68ee-4989-9069-76bed7a050e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting pydub\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub\n",
            "Successfully installed pydub-0.25.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# split audio to chunks based on silence\n",
        "# run inference on each part\n",
        "# use for large audio files\n",
        "import os\n",
        "import time\n",
        "from pydub import AudioSegment\n",
        "from pydub.silence import split_on_silence\n",
        "import subprocess\n",
        "from tqdm import tqdm\n",
        "\n",
        "os.environ[\"TMPDIR\"] = '/content/temp_dir'\n",
        "os.environ[\"PYTHONPATH\"] = \".\"\n",
        "os.environ[\"PREFIX\"] = \"INFER\"\n",
        "os.environ[\"HYDRA_FULL_ERROR\"] = \"1\"\n",
        "os.environ[\"USER\"] = \"micro\"\n",
        "\n",
        "start_time = time.time()  # Start the timer\n",
        "\n",
        "audio_path = \"/content/audio_samples/t0102.wav\"\n",
        "audio = AudioSegment.from_wav(audio_path)\n",
        "\n",
        "chunk_length_ms = 30 * 1000  # 30 seconds in milliseconds\n",
        "\n",
        "model_path = \"/content/fairseq/models_new/mms1b_fl102.pt\"\n",
        "language = \"heb\"\n",
        "\n",
        "# Split audio based on silence\n",
        "audio_chunks = split_on_silence(audio, min_silence_len=1000, silence_thresh=-40)\n",
        "\n",
        "transcripts = []  # Store the transcripts of each chunk\n",
        "\n",
        "progress_bar = tqdm(total=len(audio_chunks), desc=\"Processing Chunks\")\n",
        "\n",
        "for i, chunk in enumerate(audio_chunks):\n",
        "    # Export the current chunk to a file\n",
        "    chunk_path = f\"/content/temp_dir/chunk_{i}.wav\"\n",
        "    chunk.export(chunk_path, format=\"wav\")\n",
        "\n",
        "    # Perform inference on the current chunk\n",
        "    inference_command = f\"python examples/mms/asr/infer/mms_infer.py --model {model_path} --lang {language} --audio {chunk_path}\"\n",
        "    output = subprocess.check_output(inference_command, shell=True, text=True)\n",
        "    transcripts.append(output)\n",
        "\n",
        "    progress_bar.update(1)\n",
        "\n",
        "progress_bar.close()\n",
        "\n",
        "full_transcript = \" \".join(transcripts)  # Concatenate the transcripts\n",
        "\n",
        "end_time = time.time()  # Stop the timer\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "print(\"Full Transcript:\")\n",
        "print(full_transcript)\n",
        "print(\"Elapsed Time:\", elapsed_time, \"seconds\")\n"
      ],
      "metadata": {
        "id": "vzwnbvQo-IOx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}