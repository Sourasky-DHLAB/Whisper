{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "mount_file_id": "1QaI3_HXlKxbjLB09XiP4rNQmqaRfHIis",
      "authorship_tag": "ABX9TyNnRzA0UN5fk5JXNq/gpREt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sourasky-DHLAB/Whisper/blob/main/OtherASRs/ASR_SpeechBrain_Amharic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Library Installations"
      ],
      "metadata": {
        "id": "Rftl_GpTNmmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install speechbrain transformers librosa numpy soundfile pydub"
      ],
      "metadata": {
        "id": "UQrB82-wNljg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Library Imports"
      ],
      "metadata": {
        "id": "ueNZpYsSIuSq"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5pL7VmGVIf2Q"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries for audio processing, model inference, and utility functions.\n",
        "import time\n",
        "from pydub import AudioSegment\n",
        "from pydub.silence import split_on_silence\n",
        "from tqdm import tqdm\n",
        "from speechbrain.pretrained import EncoderASR\n",
        "import os"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Path to Audio File"
      ],
      "metadata": {
        "id": "kAcPIx_VLZGi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the path to your audio file here\n",
        "AUDIO_FILE_PATH = \"/content/drive/MyDrive/Whisper/Audio/test_2.wav\""
      ],
      "metadata": {
        "id": "tKIDTUdVLVN6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration and Model Loading"
      ],
      "metadata": {
        "id": "KWVk5BYKI1lB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Setting up the device for inference (using GPU in this case).\n",
        "run_opts = {\"device\": \"cuda\"}\n",
        "\n",
        "# Loading the pre-trained ASR model from speechbrain.\n",
        "asr_model = EncoderASR.from_hparams(\n",
        "    source=\"speechbrain/asr-wav2vec2-dvoice-amharic\",\n",
        "    savedir=\"pretrained_models/asr-wav2vec2-dvoice-amharic\",\n",
        "    run_opts=run_opts\n",
        ")"
      ],
      "metadata": {
        "id": "C4T3thSWIqR6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load and Split Audio"
      ],
      "metadata": {
        "id": "xWdV8G84Itpj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the audio from the specified path\n",
        "audio = AudioSegment.from_wav(AUDIO_FILE_PATH)\n",
        "chunk_length_ms = 30 * 1000  # 30 seconds in milliseconds\n",
        "\n",
        "# Splitting the audio file into chunks based on detected silence.\n",
        "audio_chunks = split_on_silence(audio, min_silence_len=1000, silence_thresh=-40)"
      ],
      "metadata": {
        "id": "dQoSojhlJC1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Ensure Temporary Directory Exists"
      ],
      "metadata": {
        "id": "3cmYu0DEJHdR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a temporary directory for storing audio chunks if it doesn't exist.\n",
        "if not os.path.exists(\"/content/temp_dir\"):\n",
        "    os.makedirs(\"/content/temp_dir\")"
      ],
      "metadata": {
        "id": "2b8rbkjCJL0i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Process Audio Chunks and Transcribe"
      ],
      "metadata": {
        "id": "FeCg3VKPJN1B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# List to store the transcriptions of each audio chunk.\n",
        "transcripts = []\n",
        "\n",
        "# Progress bar to visualize the processing of audio chunks.\n",
        "progress_bar = tqdm(total=len(audio_chunks), desc=\"Processing Chunks\")\n",
        "\n",
        "# Iterating over each audio chunk.\n",
        "for i, chunk in enumerate(audio_chunks):\n",
        "    # Saving the current audio chunk to a temporary file.\n",
        "    chunk_path = f\"/content/temp_dir/chunk_{i}.wav\"\n",
        "    chunk.export(chunk_path, format=\"wav\")\n",
        "\n",
        "    # Using the ASR model to transcribe the audio chunk.\n",
        "    transcription = asr_model.transcribe_file(chunk_path)\n",
        "\n",
        "    # Appending the transcription to the list.\n",
        "    transcripts.append(transcription)\n",
        "\n",
        "    # Updating the progress bar.\n",
        "    progress_bar.update(1)\n",
        "\n",
        "# Closing the progress bar after all chunks are processed.\n",
        "progress_bar.close()"
      ],
      "metadata": {
        "id": "mjmyWWi1JR85"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Compile Results and Display"
      ],
      "metadata": {
        "id": "zZyqWSkcJWES"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Combining the transcriptions of all chunks to get the full transcription.\n",
        "full_transcript = \" \".join(transcripts)\n",
        "\n",
        "# Displaying the full transcription and processing time.\n",
        "print(\"Full Transcript:\")\n",
        "print(full_transcript)"
      ],
      "metadata": {
        "id": "VvTdEUMPJcpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Save Output"
      ],
      "metadata": {
        "id": "Wa7VgVWRNK0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Extracting the base name without extension\n",
        "audio_base_name = os.path.basename(AUDIO_FILE_PATH)  # e.g., \"t0102.wav\"\n",
        "audio_name_without_ext = os.path.splitext(audio_base_name)[0]  # e.g., \"t0102\"\n",
        "\n",
        "# Saving the transcript to a .txt file with the extracted name\n",
        "txt_file_path = f\"/content/drive/MyDrive/Whisper/Transcriptions/{audio_name_without_ext}.txt\"\n",
        "\n",
        "with open(txt_file_path, 'w') as f:\n",
        "    f.write(full_transcript)"
      ],
      "metadata": {
        "id": "5-jAdemIMIUi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}