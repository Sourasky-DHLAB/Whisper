{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sourasky-DHLAB/Whisper/blob/main/Colab/Whisper_Audio.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8glStTLseMc"
      },
      "source": [
        "<div style=\"direction:rtl\" dir=\"rtl\">\n",
        "    <img src=\"http://drive.google.com/uc?export=view&id=1FnLVIqEV1Tt5rCEOIk5OxihO6xdZgTMe\"><br>\n",
        "<div style=\"direction:rtl\" dir=\"rtl\">\n",
        "  <p style=\"text-align: right; direction: rtl; float: right;\">    נכתב על ידי <a href=\"mailto:odedzarchia@tauex.tau.ac.il\">עודד זרחיה</a> | <a href=\"https://github.com/Sourasky-DHLAB\">Github</a><br>\n",
        "    <a href=\"https://cenlib.tau.ac.il/\">הספרייה המרכזית ע\"ש סוראסקי</a> | <a href=\"https://tau.ac.il/\">אוניברסיטת תל אביב</a>\n",
        "</div></p>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0G0FE9s4skOt"
      },
      "source": [
        "<div dir=\"rtl\">\n",
        "<h1><strong>1. תמלול קבצי אודיו באמצעות Whisper</strong></h1>\n",
        "<p>\n",
        "<div style=\"direction:rtl\" dir=\"rtl\">\n",
        "  <p style=\"text-align: right; direction: rtl; float: right;\">\n",
        "<a href=\"https://openai.com/blog/whisper/\"> Whisper </a>הוא מודל לזיהוי דיבור מבית <a href=\"https://openai.com/\">OpenAI</a> הזמין לציבור הרחב בקוד פתוח. מודל זה אומן על יותר מ-680 אלף שעות של שיחות באנגלית ובשפות רבות אחרות – בהן גם עברית וערבית.<br>\n",
        "מחברת זו תדריך אתכם בתמלול קטעי אודיו באמצעות Whisper. תוכלו להשתמש במחברת כפי שהיא כדי לאחסן את קבצי התמליל ב-Google Drive.<br>\n",
        "לתשומת לבכם - השימוש במחברת זו ובמודל של Whisper חופשי לחלוטין וללא שום עלות. בנוסף, אין הגבלה על אורך קטעי הוידאו/אודיו שניתן לתמלל באמצעות Whisper.\n",
        "</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fsyokosnuUdk"
      },
      "source": [
        "<h1><strong><div style=\"direction:rtl\" dir=\"rtl\">2. בדוק את סוג המעבד הגרפי (GPU) </div></strong></h1>\n",
        "<div style=\"direction:rtl\" dir=\"rtl\">\n",
        "  <p style=\"text-align: right; direction: rtl; float: right;\">סוג <a href=\"https://https://he.wikipedia.org/wiki/%D7%9E%D7%A2%D7%91%D7%93_%D7%92%D7%A8%D7%A4%D7%99\">המעבד הגרפי</a> <strong>(GPU - Graphics Processing Unit)</strong> שאתם מקבלים ב-Google Colab מגדיר את המהירות שבה קטעי האודיו יתומללו. ככל שמספר <a href=\"https://https://he.wikipedia.org/wiki/FLOPS\">הפלופס</a> <strong>(FLOPS - Floating Point Operations Per Second, פעולות נקודות צפות לשנייה)</strong> גבוה יותר, כך התמלול מהיר יותר. יחד עם זאת, גם המעבד הגרפי החלש ביותר ב-Colab מסוגל להריץ כל מודל של Whisper. יש לוודא כי בחרתם ב-GPU כמאיץ חומרה עבור מחברת זו (Runtime → Change runtime type → Hardware accelerator).</p>\n",
        "</div>\n",
        "\n",
        "<table>\n",
        "  <thead>\n",
        "    <tr>\n",
        "      <th>GPU</th>\n",
        "      <th>GPU RAM</th>\n",
        "      <th>FP32 teraFLOPS</th>\n",
        "      <th>Availability</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <td>T4</td>\n",
        "      <td>16 GB</td>\n",
        "      <td>8.1</td>\n",
        "      <td>Free</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>P100</td>\n",
        "      <td>16 GB</td>\n",
        "      <td>10.6</td>\n",
        "      <td>Colab Pro</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <td>V100</td>\n",
        "      <td>16 GB</td>\n",
        "      <td>15.7</td>\n",
        "      <td>Colab Pro (Rare)</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "\n",
        "\n",
        "<div style=\"direction:rtl\" dir=\"rtl\">\n",
        "  <p style=\"text-align: right; direction: rtl; float: right;\">\n",
        "הריצו את התא הבא כדי לוודא את סוג המעבד הגרפי שהוקצה עבור מחברת זו. אפסו את ה-runtime של המחברת אם ברצונכם לקבל מעבד גרפי אחר (Runtime → Restart runtime).</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Rr1Qg5MwuYEG",
        "outputId": "e1a97e8b-c07e-426a-a118-8a2ab93769ef",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla T4 (UUID: GPU-3ef9bdb7-d784-34e7-1551-193d2435e8e0)\n",
            "Mon Apr 17 12:15:13 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   35C    P8     9W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# check gpu type\n",
        "!nvidia-smi -L\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xRAo5l6xufyF"
      },
      "source": [
        "<div style=\"direction:rtl\" dir=\"rtl\"><h1><strong>3. התקנת ספריות</strong></h1>\n",
        "<div style=\"direction:rtl\" dir=\"rtl\">\n",
        "  <p style=\"text-align: right; direction: rtl; float: right;\">הריצו את התא הבא כדי להוריד ולהתקין את ספריית <a href=\"https://github.com/openai/whisper\">Whisper</a> הנחוצה לפעולת התמלול.</p></div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "vf4cqkCaZn0U",
        "outputId": "7b4bc780-5918-4cdc-f564-a97514403f70",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/whisper.git\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-ixxhoilj\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-ixxhoilj\n",
            "  Resolved https://github.com/openai/whisper.git to commit c09a7ae299c4c34c5839a76380ae407e7d785914\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.9/dist-packages (from openai-whisper==20230314) (2.0.0)\n",
            "Collecting ffmpeg-python==0.2.0\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.9/dist-packages (from openai-whisper==20230314) (4.65.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.9/dist-packages (from openai-whisper==20230314) (2.0.0+cu118)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.9/dist-packages (from openai-whisper==20230314) (9.1.0)\n",
            "Collecting tiktoken==0.3.1\n",
            "  Downloading tiktoken-0.3.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m60.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from openai-whisper==20230314) (1.22.4)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.9/dist-packages (from openai-whisper==20230314) (0.56.4)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.9/dist-packages (from ffmpeg-python==0.2.0->openai-whisper==20230314) (0.18.3)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.9/dist-packages (from tiktoken==0.3.1->openai-whisper==20230314) (2022.10.31)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.9/dist-packages (from tiktoken==0.3.1->openai-whisper==20230314) (2.27.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->openai-whisper==20230314) (16.0.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->openai-whisper==20230314) (3.11.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.9/dist-packages (from triton==2.0.0->openai-whisper==20230314) (3.25.2)\n",
            "Requirement already satisfied: llvmlite<0.40,>=0.39.0dev0 in /usr/local/lib/python3.9/dist-packages (from numba->openai-whisper==20230314) (0.39.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.9/dist-packages (from numba->openai-whisper==20230314) (67.6.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.9/dist-packages (from torch->openai-whisper==20230314) (1.11.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.9/dist-packages (from torch->openai-whisper==20230314) (4.5.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.9/dist-packages (from torch->openai-whisper==20230314) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.9/dist-packages (from torch->openai-whisper==20230314) (3.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken==0.3.1->openai-whisper==20230314) (2022.12.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken==0.3.1->openai-whisper==20230314) (3.4)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken==0.3.1->openai-whisper==20230314) (2.0.12)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests>=2.26.0->tiktoken==0.3.1->openai-whisper==20230314) (1.26.15)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.9/dist-packages (from jinja2->torch->openai-whisper==20230314) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.9/dist-packages (from sympy->torch->openai-whisper==20230314) (1.3.0)\n",
            "Building wheels for collected packages: openai-whisper\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20230314-py3-none-any.whl size=798038 sha256=1d35dcdc7e99599739933ea2b47299014b19ebbdfdcd1f7e1f651e32da96356c\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-pd3c51k2/wheels/fe/03/29/e7919208d11b4ab32972cb448bb84a9a675d92cd52c9a48341\n",
            "Successfully built openai-whisper\n",
            "Installing collected packages: ffmpeg-python, tiktoken, openai-whisper\n",
            "Successfully installed ffmpeg-python-0.2.0 openai-whisper-20230314 tiktoken-0.3.1\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting jiwer\n",
            "  Downloading jiwer-3.0.1-py3-none-any.whl (21 kB)\n",
            "Requirement already satisfied: click<9.0.0,>=8.1.3 in /usr/local/lib/python3.9/dist-packages (from jiwer) (8.1.3)\n",
            "Collecting rapidfuzz==2.13.7\n",
            "  Downloading rapidfuzz-2.13.7-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m45.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\n",
            "Successfully installed jiwer-3.0.1 rapidfuzz-2.13.7\n"
          ]
        }
      ],
      "source": [
        "# install libraries\n",
        "!pip install git+https://github.com/openai/whisper.git \n",
        "!pip install jiwer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rXHJxr7ru7Xr"
      },
      "source": [
        "<div style=\"direction:rtl\" dir=\"rtl\">\n",
        "  <h1><strong>4. ייבוא ספריות </strong></h1>\n",
        "  <div style=\"direction:rtl\" dir=\"rtl\">\n",
        "  <p style=\"text-align: right; direction: rtl; float: right;\">הריצו את התא הבא כדי לייבא את הספריות הנדרשות עבור פעולת התמלול.</p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "rC-mpBTgZr3D"
      },
      "outputs": [],
      "source": [
        "# import libraries\n",
        "import whisper\n",
        "from whisper.utils import get_writer\n",
        "import os\n",
        "import string\n",
        "from jiwer import wer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R4K42pokvEGa"
      },
      "source": [
        "<h1><strong><div style=\"direction:rtl\" dir=\"rtl\">5. חיבור ל-Google Drive </div></strong></h1>\n",
        "<div style=\"direction:rtl\" dir=\"rtl\">\n",
        "  <p style=\"text-align: right; direction: rtl; float: right;\">הריצו את התא הבא כדי להתחבר ל-Google Drive האישי שלכם. כדי לראות את הקבצים שלכם פתחו את סייר הקבצים בתפריט השמאלי.</p></div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9LpaqE6ePPIK",
        "outputId": "b91687d6-cbc5-479b-e6de-a1fc9ce7341f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\", force_remount=True) # This will prompt for authorization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yDtp_AGUvPiS"
      },
      "source": [
        "<h1 dir=\"rtl\"><strong>6. הגדרת תיקיות </strong></h1>\n",
        "<div style=\"direction:rtl\" dir=\"rtl\">\n",
        "  <p style=\"text-align: right; direction: rtl; float: right;\">הריצו את התא הבא כדי להגדיר את מיקום קבצי האודיו והתמליל:<br>\n",
        "    <ol style=\"float:right;\">\n",
        "      <li>קבצי אודיו - מחברת זו מניחה כי קבצי האודיו לתמלול נמצאים ב-Google Drive תחת תיקיית <code>Whisper/Audio/</code>. ניתן לשנות הגדרה זו במשתנה <code>audio_folder_path</code>.</li>\n",
        "      <li>קבצי תמליל - מחברת זו מניחה כי קבצי הטקסט של התמלול יישמרו ב-Google Drive תחת תיקיית <code>Whisper/Transcriptions/</code>. ניתן לשנות הגדרה זו במשתנה <code>transcription_folder_path</code>. במידה ואינה קיימת, תיקייה זו תיווצר באופן אוטומטי לאחר הרצת התא.</li>\n",
        "    </ol>\n",
        "  </p>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "e_fKqCK9OQLE"
      },
      "outputs": [],
      "source": [
        "# Assuming the audio files are in a folder called \"Audio\" under \"Whisper\"\n",
        "audio_folder_path = \"/content/drive/MyDrive/Whisper/Audio/\"\n",
        "\n",
        "# Assuming the text files will be placed in a folder called \"Transcriptions\" under \"Whisper\"\n",
        "transcription_folder_path = \"/content/drive/MyDrive/Whisper/Transcriptions/\"\n",
        "\n",
        "# Create \"Transcriptions\" folder if does not exist\n",
        "if not os.path.exists(transcription_folder_path):\n",
        "  os.makedirs(transcription_folder_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PyZ4xlequt4r"
      },
      "source": [
        "<h1><strong><div style=\"direction:rtl\" dir=\"rtl\">7. טעינת קבצי אודיו לתמלול </div></h1></strong>\n",
        "<div style=\"direction:rtl\" dir=\"rtl\">\n",
        "  <p style=\"text-align: right; direction: rtl; float: right;\">\n",
        "הריצו את התא הבא כדי לטעון את קבצי האודיו הנמצאים בתיקיית <code>Whisper/Audio/</code> לתוך משתנה מסוג <a href=\"https://he.wikipedia.org/wiki/%D7%A8%D7%A9%D7%99%D7%9E%D7%94_(%D7%9E%D7%91%D7%A0%D7%94_%D7%A0%D7%AA%D7%95%D7%A0%D7%99%D7%9D)\">רשימה</a> (list). הפלט יציג את שמות וכמות הקבצים לתמלול.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Lr0_KI8N0mWY",
        "outputId": "fd378e50-3cb5-4b70-da77-1125388c79ce",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Whisper/Audio/tali.mp4\n",
            "\u001b[1m There are 1 audio files to transcribe\n"
          ]
        }
      ],
      "source": [
        "# Get a list of all the file paths in the folder\n",
        "audio_files = []\n",
        "for file in os.listdir(audio_folder_path):\n",
        "    audio_files.append(audio_folder_path + file)\n",
        "for p in audio_files:\n",
        "    print(p)\n",
        "print(f\"\\033[1m There are {len(audio_files)} audio files to transcribe\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FDOniT4Y61aH"
      },
      "source": [
        "<h1 id=\"-div-style-direction-rtl-dir-rtl-8-div-\"><strong><div style=\"direction:rtl\" dir=\"rtl\">8. טעינת מודל התמלול </div></strong></h1>\n",
        "<div style=\"direction:rtl\" dir=\"rtl\">\n",
        "  <p style=\"text-align: right; direction: rtl; float: right;\">הריצו את התא הבא כדי לטעון את מודל התמלול המתאים. יושם לב כי אנו משתמשים במודל הגדול (<strong>large</strong>), המדויק ביותר עבור השפה העברית. <br> ישנם חמישה מודלים, ארבעה עם גרסאות באנגלית בלבד, המציעים פשרות שונות בין מהירות לדיוק. יושם לב כי פעולת התמלול צורכת משאבי מחשוב מרובים. לפיכך, אם ברצונכם להשתמש במודל הגדול עבור קבצים מרובים, מומלץ להריץ את המחברת בסביבה עם משאבי מחשוב מובטחים כדוגמת <a href=\"https://colab.research.google.com/signup\">Google Colab Pro</a>. עבור תמלול באנגלית בלבד, מודלי ה-en. נוטים לתפקד טוב יותר. <br>בטבלה שלהלן מוצגים שמות המודלים הזמינים יחד עם דרישות הזיכרון והמהירות היחסית שלהם:\n",
        "</div>\n",
        "\n",
        "<table>\n",
        "<thead>\n",
        "<tr>\n",
        "<th style=\"text-align:center\">Size</th>\n",
        "<th style=\"text-align:center\">Parameters</th>\n",
        "<th style=\"text-align:center\">English-only model</th>\n",
        "<th style=\"text-align:center\">Multilingual model</th>\n",
        "<th style=\"text-align:center\">Required VRAM</th>\n",
        "<th style=\"text-align:center\">Relative speed</th>\n",
        "</tr>\n",
        "</thead>\n",
        "<tbody>\n",
        "<tr>\n",
        "<td style=\"text-align:center\">tiny</td>\n",
        "<td style=\"text-align:center\">39 M</td>\n",
        "<td style=\"text-align:center\"><code>tiny.en</code></td>\n",
        "<td style=\"text-align:center\"><code>tiny</code></td>\n",
        "<td style=\"text-align:center\">~1 GB</td>\n",
        "<td style=\"text-align:center\">~32x</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"text-align:center\">base</td>\n",
        "<td style=\"text-align:center\">74 M</td>\n",
        "<td style=\"text-align:center\"><code>base.en</code></td>\n",
        "<td style=\"text-align:center\"><code>base</code></td>\n",
        "<td style=\"text-align:center\">~1 GB</td>\n",
        "<td style=\"text-align:center\">~16x</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"text-align:center\">small</td>\n",
        "<td style=\"text-align:center\">244 M</td>\n",
        "<td style=\"text-align:center\"><code>small.en</code></td>\n",
        "<td style=\"text-align:center\"><code>small</code></td>\n",
        "<td style=\"text-align:center\">~2 GB</td>\n",
        "<td style=\"text-align:center\">~6x</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"text-align:center\">medium</td>\n",
        "<td style=\"text-align:center\">769 M</td>\n",
        "<td style=\"text-align:center\"><code>medium.en</code></td>\n",
        "<td style=\"text-align:center\"><code>medium</code></td>\n",
        "<td style=\"text-align:center\">~5 GB</td>\n",
        "<td style=\"text-align:center\">~2x</td>\n",
        "</tr>\n",
        "<tr>\n",
        "<td style=\"text-align:center\">large</td>\n",
        "<td style=\"text-align:center\">2870 M</td>\n",
        "<td style=\"text-align:center\">N/A</td>\n",
        "<td style=\"text-align:center\"><code>large</code></td>\n",
        "<td style=\"text-align:center\">~10 GB</td>\n",
        "<td style=\"text-align:center\">1x</td>\n",
        "</tr>\n",
        "</tbody>\n",
        "</table>\n",
        "<p><div style=\"direction:rtl\" dir=\"rtl\"><br></p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "ToWWQiTv7foY",
        "outputId": "2a46d635-c8c4-4393-f0f3-7ca8b6483678",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|█████████████████████████████████████| 1.42G/1.42G [00:15<00:00, 99.4MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m Model loaded successfully\n"
          ]
        }
      ],
      "source": [
        "# load whisper model\n",
        "model = whisper.load_model(\"medium.en\")\n",
        "print(f\"\\033[1m Model loaded successfully\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-D_Xnwk0_ZIG"
      },
      "source": [
        "<h1 id=\"-div-style-direction-rtl-dir-rtl-9-div-\"><strong><div style=\"direction:rtl\" dir=\"rtl\">9. קביעת שפה לתמלול וסוג הפלט</div></strong></h1>\n",
        "<div style=\"direction:rtl\" dir=\"rtl\">\n",
        " <p style=\"text-align: right; direction: rtl; float: right;\">הריצו את התא הבא כדי לקבוע שני משתנים:\n",
        "<ol style=\"float:right;\">\n",
        "  <li><code>lang</code> - הגדרת שפת התמלול. <code>he</code> משמעו עברית. ניתן להדפיס את רשימת השפות הנתמכת על ידי הרצת הפקודה <code>print(whisper.tokenizer.LANGUAGES)</code> בתא נפרד.</li>\n",
        "  <li><code>output_format</code> - הגדרת סוג הפלט. Whisper תומך בהפקת הפלטים הבאים:\n",
        "    <ul>\n",
        "      <li><code>txt</code> - קובץ טקסט עם מעברי שורות, ניתן לפתיחה בכל מעבד תמלילים.</li>\n",
        "      <li><code>srt/vtt</code>- קבצי כתוביות המכילים טקסט וחתימות זמן.</li>\n",
        "      <li><code>tsv</code>- קובץ טקסט מופרד בטאבים עם חלוקה לסגמנטים. ניתן לפתיחה כגיליון אלקטרוני.</li>\n",
        "      <li><code>json</code>- מבנה מידע המורכב מזוגות של מפתח-ערך.</li>\n",
        "    </ul><br>\n",
        "    החליפו את ערך המשתנה בהתאם לסוג הפלט שברצונכם לקבל. ניתן להפיק את כל סוגי הפלטים באמצעות קביעת הערך <code>all</code>.</p>\n",
        "  </li>\n",
        "</ol>\n",
        "</div>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "ZG7xbllv_xvO"
      },
      "outputs": [],
      "source": [
        "lang = 'en'\n",
        "output_format = 'all'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Elbd5zmB-_1a"
      },
      "source": [
        "<h1 id=\"-div-style-direction-rtl-dir-rtl-10-div-\"><strong><div style=\"direction:rtl\" dir=\"rtl\">10. תמלול </div></strong></h1>\n",
        "<div style=\"direction:rtl\" dir=\"rtl\">\n",
        "  <p style=\"text-align: right; direction: rtl; float: right;\">\n",
        "הריצו את התא הבא כדי לתמלל את קבצי האודיו בתיקיית <code>Whisper/Audio</code>. \n",
        "קבצי הטקסט יישמרו בתיקיית <code>Whisper/Transcriptions/</code> עם שם זהה לקבצי האודיו אך עם סיומת txt. </p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adgPhhduOSyz"
      },
      "outputs": [],
      "source": [
        "# set timer\n",
        "import time\n",
        "start_time = time.time()\n",
        "# transcribe audio files in list\n",
        "for p in audio_files:    \n",
        "  result = model.transcribe(p, verbose = False, language = lang) # to translate add task = 'transalte'\n",
        "  options = {\n",
        "        \"max_line_width\": None,\n",
        "        \"max_line_count\": None,\n",
        "        \"highlight_words\": False\n",
        "    }\n",
        "  # use get_writer method to output files\n",
        "  output_file = get_writer(output_format, transcription_folder_path)\n",
        "  output_file(result, p[:-4], options=options)\n",
        "  print(p)\n",
        "print(f\"\\033[1m--- Transcribed {len(audio_files)} audio files in %s seconds ---\" % (time.time() - start_time))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gNRHYaXRutE"
      },
      "source": [
        "<h1 id=\"-div-style-direction-rtl-dir-rtl-11-div-\"><strong><div style=\"direction:rtl\" dir=\"rtl\">11. בדיקת איכות (אופציונלי) </div></strong></h1>\n",
        "<div style=\"direction:rtl\" dir=\"rtl\">\n",
        "\n",
        "<p style=\"text-align: right; direction: rtl; float: right;\">הרצת התאים הבאים תאפשר לכם להעריך את הדיוק של התמלול שהפקתם באמצעות Whisper. בדיקה זו מתבצעת במספר שלבים, ומצריכה את קובץ התמלול המקורי (<a href=\"https://https://en.wikipedia.org/wiki/Ground_truth\">Ground Truth</a>): </p>\n",
        "<ol>\n",
        "<li>נירמול פלט התמלול של Whisper וקובץ התמלול המקורי  - הסרת סימני פיסוק, רווחים מיותרים, קפיטליזציה וכו&#39;. הנתיב של הקובץ שהפקתם באמצעות Whisper מוגדר במשתנה מוגדר במשתנה <code>whisper_output</code>. הנתיב של קובץ התמלול המקורי מוגדר במשתנה <code>ground_truth</code>.</li>\n",
        "<li>השוואה בין שני הקבצים וחישוב אחוז השגיאות בהתאם למדד <a href=\"https://https://en.wikipedia.org/wiki/Word_error_rate\">Word Error Rate</a> - באמצעות ספריית jiwer.</li>\n",
        "</ol>\n",
        "<p style=\"text-align: right; direction: rtl; float: right;\">הפלט המתקבל מוצג באחוזים ומצביע על אחוז המילים השגויות בקובץ התמלול, וזאת בהשוואה לקובץ המקור. כלומר, במידה ואחוז השגיאות עומד על כ-3%, אזי מתוך 100 מילים יש 3 מילים שגויות.</p>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sL1NPgEhSONs"
      },
      "outputs": [],
      "source": [
        "# normalize whisper output\n",
        "whisper_output = '/content/drive/MyDrive/Whisper/Transcriptions/Copy of Snyder.txt'\n",
        "whisper_output_norm = open(whisper_output, 'r').read().lower().translate(str.maketrans('', '', string.punctuation)).replace('\\n', ' ')\n",
        "print(whisper_output_norm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zTOY_r7kUsBc"
      },
      "outputs": [],
      "source": [
        "# normalize ground truth\n",
        "ground_truth = '/content/drive/MyDrive/Whisper/GT/Snyder_GT.txt'\n",
        "ground_truth_norm = open(ground_truth, 'r').read().lower().translate(str.maketrans('', '', string.punctuation)).replace('\\n', ' ')\n",
        "print(ground_truth_norm)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w6mR6ErlVUwD"
      },
      "outputs": [],
      "source": [
        "# calculate WER\n",
        "reference = ground_truth_norm\n",
        "hypothesis = whisper_output_norm\n",
        "error = wer(reference, hypothesis)\n",
        "error_percentage = \"{:.0%}\".format(error)\n",
        "print(error_percentage)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "mount_file_id": "https://github.com/Sourasky-DHLAB/Whisper/blob/main/Whisper_Audio.ipynb",
      "authorship_tag": "ABX9TyPBg2TPwtxx59mrQly2AQxN",
      "include_colab_link": true
    },
    "gpuClass": "premium",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}